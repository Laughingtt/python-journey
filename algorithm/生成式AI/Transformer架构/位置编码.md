# Transformer位置嵌入
<!-- TOC -->
* [Transformer位置嵌入](#transformer位置嵌入)
  * [为什么需要位置嵌入？](#为什么需要位置嵌入)
    * [位置嵌入的实现](#位置嵌入的实现)
    * [具体步骤示例](#具体步骤示例)
  * [位置编码公式生成的编码向量，并给出一个完整的示例](#位置编码公式生成的编码向量并给出一个完整的示例)
    * [位置编码公式](#位置编码公式)
    * [示例计算](#示例计算)
      * [位置 0 的位置编码：](#位置-0-的位置编码)
      * [位置 1 的位置编码：](#位置-1-的位置编码)
      * [位置 2 的位置编码：](#位置-2-的位置编码)
    * [完整的编码向量](#完整的编码向量)
  * [位置编码中使用正弦（sin）和余弦（cos）函数主要有以下几个原因：](#位置编码中使用正弦sin和余弦cos函数主要有以下几个原因)
    * [1. **周期性函数的特性**：](#1-周期性函数的特性)
    * [2. **不同频率的编码**：](#2-不同频率的编码)
    * [3. **易于模型学习**：](#3-易于模型学习)
    * [4. **保留位置信息的区别性**：](#4-保留位置信息的区别性)
    * [5. **捕捉相对位置**：](#5-捕捉相对位置)
    * [6. **无需学习位置编码参数**：](#6-无需学习位置编码参数)
    * [示例说明：](#示例说明)
<!-- TOC -->


Transformer架构中的位置嵌入（positional
embedding）是用于向模型提供序列中每个元素的位置信息的一种技术。因为Transformer架构本质上是无序的，它不直接处理序列数据，所以需要通过位置嵌入来引入位置信息。下面是对其工作原理的通俗解释：

## 为什么需要位置嵌入？

Transformer模型的自注意力机制在处理输入序列时，每个元素可以与其他任何元素进行交互，而不关心它们在序列中的具体位置。这种机制虽然强大，但会丢失序列的位置信息。因此，需要一种方法来将位置信息注入到模型中，以便它能够理解序列中元素的顺序。

### 位置嵌入的实现

1. **生成位置编码（Positional Encoding）**：
   位置嵌入通过生成一组固定的位置编码来实现。常用的方法是使用正弦和余弦函数来生成这些编码。具体来说，对于序列中的第 \(
   pos \) 个位置和嵌入向量的第 \( i \) 个维度，位置编码计算公式如下：

   $$
   PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
   $$
   $$
   PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
   $$

   其中， \( d_{\text{model}} \) 是嵌入向量的维度。这个公式生成的编码在不同位置上是不同的，并且具有平滑的变化特性。

2. **将位置编码与输入嵌入相加**：
    - 输入序列中的每个元素首先通过嵌入层转换为一个向量，称为输入嵌入（Input Embedding）。
    - 然后，将相应位置的编码向量添加到输入嵌入向量中，形成位置感知的嵌入向量。

### 具体步骤示例


假设我们有一个序列长度为4，嵌入维度为8的简单示例：

1. **输入序列**：
   \[ ["我", "爱", "学习", "AI"] \]

2. **输入嵌入**：
   假设嵌入层将这些词转化为以下嵌入向量（随机示例）：
   \[
   \begin{align*}
   \text{“我”} & \rightarrow [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8] \\
   \text{“爱”} & \rightarrow [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] \\
   \text{“学习”} & \rightarrow [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] \\
   \text{“AI”} & \rightarrow [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1]
   \end{align*}
   \]

3. **位置编码**：
   生成位置编码向量（举例）：
   \[
   \begin{align*}
   PE(0) & \rightarrow [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0] \\
   PE(1) & \rightarrow [0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6] \\
   PE(2) & \rightarrow [0.9, 0.4, 0.9, 0.4, 0.9, 0.4, 0.9, 0.4] \\
   PE(3) & \rightarrow [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
   \end{align*}
   \]

4. **位置感知嵌入**：
   将位置编码加到输入嵌入上：
   \[
   \begin{align*}
   \text{“我”} & \rightarrow [0.1+0.0, 0.2+1.0, 0.3+0.0, 0.4+1.0, 0.5+0.0, 0.6+1.0, 0.7+0.0, 0.8+1.0] \\
   \text{“爱”} & \rightarrow [0.2+0.8, 0.3+0.6, 0.4+0.8, 0.5+0.6, 0.6+0.8, 0.7+0.6, 0.8+0.8, 0.9+0.6] \\
   \text{“学习”} & \rightarrow [0.3+0.9, 0.4+0.4, 0.5+0.9, 0.6+0.4, 0.7+0.9, 0.8+0.4, 0.9+0.9, 1.0+0.4] \\
   \text{“AI”} & \rightarrow [0.4+0.5, 0.5+0.5, 0.6+0.5, 0.7+0.5, 0.8+0.5, 0.9+0.5, 1.0+0.5, 1.1+0.5]
   \end{align*}
   \]

这样，每个输入嵌入都携带了位置信息，使得Transformer可以更好地理解序列的结构和顺序。这就是位置嵌入在Transformer中的实现流程。



## 位置编码公式生成的编码向量，并给出一个完整的示例

### 位置编码公式

位置编码公式如下：
\[
PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\]
\[
PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\]

其中：
- \( pos \) 是位置索引。
- \( i \) 是嵌入向量的维度索引。
- \( d_{\text{model}} \) 是嵌入向量的维度。

### 示例计算

假设：
- 序列的长度为 3（例如句子 "I love AI" 有3个token）。
- 嵌入向量的维度 \( d_{\text{model}} = 8 \)。

我们需要计算位置 0、1、2 对应的 8 维位置编码。

#### 位置 0 的位置编码：

1. \( i = 0 \):
   \[
   PE(0, 0) = \sin\left(\frac{0}{10000^{0/8}}\right) = \sin(0) = 0
   \]
   \[
   PE(0, 1) = \cos\left(\frac{0}{10000^{0/8}}\right) = \cos(0) = 1
   \]

2. \( i = 1 \):
   \[
   PE(0, 2) = \sin\left(\frac{0}{10000^{2/8}}\right) = \sin(0) = 0
   \]
   \[
   PE(0, 3) = \cos\left(\frac{0}{10000^{2/8}}\right) = \cos(0) = 1
   \]

3. \( i = 2 \):
   \[
   PE(0, 4) = \sin\left(\frac{0}{10000^{4/8}}\right) = \sin(0) = 0
   \]
   \[
   PE(0, 5) = \cos\left(\frac{0}{10000^{4/8}}\right) = \cos(0) = 1
   \]

4. \( i = 3 \):
   \[
   PE(0, 6) = \sin\left(\frac{0}{10000^{6/8}}\right) = \sin(0) = 0
   \]
   \[
   PE(0, 7) = \cos\left(\frac{0}{10000^{6/8}}\right) = \cos(0) = 1
   \]

所以位置 0 的编码向量为：
\[
[0, 1, 0, 1, 0, 1, 0, 1]
\]

#### 位置 1 的位置编码：

1. \( i = 0 \):
   \[
   PE(1, 0) = \sin\left(\frac{1}{10000^{0/8}}\right) = \sin(1)
   \]
   \[
   PE(1, 1) = \cos\left(\frac{1}{10000^{0/8}}\right) = \cos(1)
   \]

2. \( i = 1 \):
   \[
   PE(1, 2) = \sin\left(\frac{1}{10000^{2/8}}\right) = \sin\left(\frac{1}{10000^{0.25}}\right)
   \]
   \[
   PE(1, 3) = \cos\left(\frac{1}{10000^{2/8}}\right) = \cos\left(\frac{1}{10000^{0.25}}\right)
   \]

3. \( i = 2 \):
   \[
   PE(1, 4) = \sin\left(\frac{1}{10000^{4/8}}\right) = \sin\left(\frac{1}{10000^{0.5}}\right)
   \]
   \[
   PE(1, 5) = \cos\left(\frac{1}{10000^{4/8}}\right) = \cos\left(\frac{1}{10000^{0.5}}\right)
   \]

4. \( i = 3 \):
   \[
   PE(1, 6) = \sin\left(\frac{1}{10000^{6/8}}\right) = \sin\left(\frac{1}{10000^{0.75}}\right)
   \]
   \[
   PE(1, 7) = \cos\left(\frac{1}{10000^{6/8}}\right) = \cos\left(\frac{1}{10000^{0.75}}\right)
   \]

所以位置 1 的编码向量为：
\[
[\sin(1), \cos(1), \sin(0.01), \cos(0.01), \sin(0.001), \cos(0.001), \sin(0.0001), \cos(0.0001)]
\]

（注：由于实际计算结果依赖于具体的数值计算，我们使用 \(\sin\) 和 \(\cos\) 的具体数值）

#### 位置 2 的位置编码：

1. \( i = 0 \):
   \[
   PE(2, 0) = \sin\left(\frac{2}{10000^{0/8}}\right) = \sin(2)
   \]
   \[
   PE(2, 1) = \cos\left(\frac{2}{10000^{0/8}}\right) = \cos(2)
   \]

2. \( i = 1 \):
   \[
   PE(2, 2) = \sin\left(\frac{2}{10000^{2/8}}\right) = \sin\left(\frac{2}{10000^{0.25}}\right)
   \]
   \[
   PE(2, 3) = \cos\left(\frac{2}{10000^{2/8}}\right) = \cos\left(\frac{2}{10000^{0.25}}\right)
   \]

3. \( i = 2 \):
   \[
   PE(2, 4) = \sin\left(\frac{2}{10000^{4/8}}\right) = \sin\left(\frac{2}{10000^{0.5}}\right)
   \]
   \[
   PE(2, 5) = \cos\left(\frac{2}{10000^{4/8}}\right) = \cos\left(\frac{2}{10000^{0.5}}\right)
   \]

4. \( i = 3 \):
   \[
   PE(2, 6) = \sin\left(\frac{2}{10000^{6/8}}\right) = \sin\left(\frac{2}{10000^{0.75}}\right)
   \]
   \[
   PE(2, 7) = \cos\left(\frac{2}{10000^{6/8}}\right) = \cos\left(\frac{2}{10000^{0.75}}\right)
   \]

所以位置 2 的编码向量为：
\[
[\sin(2), \cos(2), \sin(0.02), \cos(0.02), \sin(0.002), \cos(0.002), \sin(0.0002), \cos(0.0002)]
\]

### 完整的编码向量

综合上述计算，位置编码向量如下：

- 位置 0 的编码向量：
  \[
  [0, 1, 0, 1, 0, 1, 0, 1]
  \]

- 位置 1 的编码向量：
  \[
  [\sin(1), \cos(1), \sin(0.01), \cos(0.01), \sin(0.001), \cos(0.001), \sin(0.0001), \cos(0.0001)]
  \]

- 位置 2 的编码向量：
  \[
  [\sin(2), \cos(2), \sin(0.02), \cos(0.02), \sin(0.002), \cos(0.002), \sin(0.0002), \cos(0.0002)]
  \]

这样，我们得到了序列中每个位置的8维位置编码，这些编码向量将被加到对应的token嵌入向量上，使得Transformer模型能够感知到token在序列中的位置。



## 位置编码中使用正弦（sin）和余弦（cos）函数主要有以下几个原因：

### 1. **周期性函数的特性**：
正弦和余弦函数是周期性函数，这意味着它们可以在一定范围内重复。这种特性使得它们非常适合表示序列中的位置，因为在很多情况下，我们需要模型对长序列中的某些位置具有一定的重复感知能力。

### 2. **不同频率的编码**：
使用正弦和余弦函数的不同频率可以帮助模型在不同的尺度上捕捉位置信息。通过调节频率参数，位置编码能够对序列中的局部和全局位置信息进行编码。公式中的 \( \frac{1}{10000^{2i/d_{\text{model}}}} \) 就是为了产生不同的频率。

### 3. **易于模型学习**：
正弦和余弦函数的输出在 \([-1, 1]\) 之间，这种平滑且连续的变化使得模型可以更容易地学习和泛化位置信息。因为这些函数在输入位置变化时的输出变化是平滑的，所以对于不同位置之间的差异，模型可以更自然地捕捉。

### 4. **保留位置信息的区别性**：
对于每个位置，正弦和余弦函数生成的编码向量在高维空间中是唯一的。这意味着即使位置非常接近，它们的编码向量仍然是不同的，这有助于模型区分序列中的不同位置。

### 5. **捕捉相对位置**：
由于正弦和余弦函数的特性，使用这些函数生成的编码向量包含了位置信息的相对关系。模型可以利用这些信息来理解序列中不同位置之间的相对关系，而不仅仅是绝对位置。

### 6. **无需学习位置编码参数**：
与可学习的位置编码不同，正弦和余弦函数生成的位置编码是固定的，不需要训练。这减少了模型的参数量，同时提供了可靠的位置信息，使模型在处理序列时能够更好地利用位置信息。

### 示例说明：

假设嵌入向量的维度 \( d_{\text{model}} = 8 \)，我们可以计算位置0和位置1的部分位置编码：

对于位置0（pos = 0）和位置1（pos = 1），我们计算第0维和第1维的编码：

- 位置0，第0维和第1维：
  \[
  PE(0, 0) = \sin\left(\frac{0}{10000^{0/8}}\right) = \sin(0) = 0
  \]
  \[
  PE(0, 1) = \cos\left(\frac{0}{10000^{0/8}}\right) = \cos(0) = 1
  \]

- 位置1，第0维和第1维：
  \[
  PE(1, 0) = \sin\left(\frac{1}{10000^{0/8}}\right) = \sin(1)
  \]
  \[
  PE(1, 1) = \cos\left(\frac{1}{10000^{0/8}}\right) = \cos(1)
  \]

我们可以看到，位置0和位置1的编码在高维空间中是不同的，且随着位置的增加，正弦和余弦函数的输出会在不同频率下变化，使得编码向量具有独特性和周期性。

这种方式保证了位置编码向量既能提供丰富的位置信息，又能被模型轻松地学习和利用。